# ğŸ“ˆ Real-Time Stock Data Pipeline with Kafka, Python, and MongoDB

This project fetches real-time stock data using the [Finnhub API](https://finnhub.io/), streams it using **Apache Kafka**, and stores it in **MongoDB** for further analysis or processing. Kafka is deployed using **Docker**, and all ingestion and processing logic is written in **Python**.

---

## ğŸš€ Features

- Live stock market data streaming using WebSocket
- Kafka-based data ingestion pipeline
- Docker-based Kafka broker setup
- Python-based producer and consumer services
- Data storage in MongoDB

---

## ğŸ—‚ï¸ Project Structure
â”‚
â”œâ”€â”€ docker-compose.yml             # Kafka, Zookeeper, MongoDB services
â”œâ”€â”€ kafka/
â”‚   â””â”€â”€ create_topic.sh            # Script to create Kafka topic
â”‚
â”œâ”€â”€ producer/
â”‚   â”œâ”€â”€ producer.py                # Connects to Finnhub WebSocket and produces to Kafka
â”‚   â””â”€â”€ .env                       # API key and bootstrap server details
â”‚
â”œâ”€â”€ consumer/
â”‚   â””â”€â”€ consumer_kafka_mongo.py    # Consumes from Kafka and inserts into MongoDB
â”‚
â”œâ”€â”€ requirements.txt               # Python dependencies
â””â”€â”€ README.md                      # You are here

---

## ğŸ³ Docker Setup for Kafka & MongoDB

### Step 1: Start Kafka and MongoDB

```bash
docker-compose up -d

docker exec -it kafka /bin/bash
cd /opt/kafka/bin
./kafka-topics.sh --create --topic stock-data --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1

ğŸ“¬ Sample Output

Producer:
Produced: {'type': 'trade', 'data': [{'p': 134.5, 's': 'AAPL', ...}]}

Consumer:
âœ” Received: {'type': 'trade', 'data': [...]} â†’ Inserted into MongoDB

```
---

ğŸ™Œ Acknowledgements
    â€¢   Finnhub.io
	â€¢	Apache Kafka
	â€¢	MongoDB